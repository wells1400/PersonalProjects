{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3.7\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新闻语料Word2Vec处理，标准普尔数据打上标签\n",
    "class DataProcess:\n",
    "    def __init__(self, \n",
    "                 dow_jons_path=r'E:\\SIOA\\Program\\PersonalProfit\\DowJonesPredict\\SourceData\\DowJones.csv',\n",
    "                 news_path=r'E:\\SIOA\\Program\\PersonalProfit\\DowJonesPredict\\SourceData\\News.csv',\n",
    "                 save_model_file=r'E:\\SIOA\\Program\\PersonalProfit\\DowJonesPredict\\SourceData\\\\',\n",
    "                 padding_size = 1500,\n",
    "                 vector_size = 64\n",
    "                ):\n",
    "        self.dow_jons_path = dow_jons_path\n",
    "        self.news_path = news_path\n",
    "        # 初始化两个数据表格\n",
    "        self.dow_jons_pd,self.news_pd = self.read_files()\n",
    "        # 词向量模型\n",
    "        self.WordVectorModel = None\n",
    "        # padding size\n",
    "        self.padding_size = padding_size\n",
    "        # vector size\n",
    "        self.vector_size = vector_size\n",
    "        # 代表每天新闻情况的矩阵\n",
    "        self.news_matrix = []\n",
    "        self.save_model_file=save_model_file\n",
    "        \n",
    "    # 读取新闻语料和dowjons数据，整合日期\n",
    "    def read_files(self):\n",
    "        news_pd = pd.read_csv(self.news_path)\n",
    "        dow_jons_pd = pd.read_csv(self.dow_jons_path)\n",
    "        news_pd = news_pd.loc[news_pd.Date.isin(dow_jons_pd.Date)]\n",
    "        return dow_jons_pd, news_pd\n",
    "        \n",
    "     # 输入text,进行缩写词拓展，去除停用词处理\n",
    "    def news_process(self, text, remove_stopwords=False):\n",
    "        # 全部小写\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'b\\\"', ' ', text)\n",
    "        text = re.sub(r'b\\'', ' ', text)\n",
    "        text = re.sub(r'\\'', '', text)\n",
    "        text = re.sub(r'\\\"', '', text)\n",
    "        # 替换不规则的词\n",
    "        text = re.sub(r'&amp;', '', text) \n",
    "        text = re.sub(r'0,0', '00', text) \n",
    "        text = re.sub(r'[_\"\\-;%()|.,+&=*%.,!?:#@\\[\\]]', ' ', text)\n",
    "        text = re.sub(r'\\'', ' ', text)\n",
    "        text = re.sub(r'\\$', ' $ ', text)\n",
    "        text = re.sub(r'u s ', ' united states ', text)\n",
    "        text = re.sub(r'u n ', ' united nations ', text)\n",
    "        text = re.sub(r'u k ', ' united kingdom ', text)\n",
    "        text = re.sub(r'j k ', ' jk ', text)\n",
    "        text = re.sub(r' s ', ' ', text)\n",
    "        text = re.sub(r' yr ', ' year ', text)\n",
    "        text = re.sub(r' l g b t ', ' lgbt ', text)\n",
    "        text = re.sub(r'0km ', '0 km ', text)\n",
    "        \n",
    "        # 将缩写词进行拓展\n",
    "        contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}\n",
    "        new_text = []\n",
    "        text = text.split()\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "                continue\n",
    "            new_text.append(word)\n",
    "        if remove_stopwords:\n",
    "            stops_set = set(stopwords.words(\"english\"))\n",
    "            new_text = [word for word in new_text if word not in stops_set]\n",
    "        return \" \".join(new_text)\n",
    "    \n",
    "    # 新闻语料数据进行清洗，去除停用词处理\n",
    "    # 添加DataFrame 中Cleaned_News列\n",
    "    def clean_news_text(self):\n",
    "        news_text_list = []\n",
    "        for text in self.news_pd.News:\n",
    "            news_text_list.append(self.news_process(text, remove_stopwords=True))\n",
    "        self.news_pd['Cleaned_News'] = news_text_list\n",
    "        # 当日所有新闻首尾连接\n",
    "        dates_list = list(self.news_pd.Date.drop_duplicates())\n",
    "        news_list = []\n",
    "        for date in dates_list:\n",
    "            selected_pd = self.news_pd.loc[self.news_pd.Date == date]\n",
    "            news_list.append(\" \".join(list(selected_pd.Cleaned_News)))\n",
    "        new_data_pd = pd.DataFrame({'Date':dates_list, 'Cleaned_News':news_list})\n",
    "        self.news_pd = new_data_pd\n",
    "        # 将清理好的新闻输出成果文档\n",
    "        with open(self.save_model_file + r'Cleaned_news.txt','w') as f:\n",
    "            for senteces in self.news_pd.Cleaned_News:\n",
    "                f.write(senteces)\n",
    "                f.write('\\n')\n",
    "        \n",
    "    # Word2vec训练\n",
    "    def model_train(self): \n",
    "        # model_file_name为训练语料的路径,save_model为保存模型名\n",
    "        # 模型训练，生成词向量\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "        sentences = word2vec.Text8Corpus(self.save_model_file + r'Cleaned_news.txt')  # 加载语料\n",
    "        self.WordVectorModel = gensim.models.Word2Vec(sentences, size=self.vector_size)  # 训练模型; \n",
    "        self.WordVectorModel.save(self.save_model_file + r'WordVector.model')\n",
    "        self.WordVectorModel.wv.save_word2vec_format(self.save_model_file + \"WordVectorModel.bin\", binary=True)\n",
    "        \n",
    "    # 将每天的新闻转成一个固定大小的矩阵\n",
    "    def transfer_news_to_matrix(self):\n",
    "        res_matrix = []\n",
    "        for sentence in self.news_pd.Cleaned_News[:-1]:\n",
    "            news_matrix = []\n",
    "            word_list = sentence.split()\n",
    "            for i in range(len(word_list)):\n",
    "                if word_list[i] in self.WordVectorModel:\n",
    "                    news_matrix.append(self.WordVectorModel[word_list[i]].tolist())\n",
    "                if len(news_matrix) >= self.padding_size:\n",
    "                    break\n",
    "            while len(news_matrix) < self.padding_size:\n",
    "                news_matrix.append([0] * self.vector_size)\n",
    "            res_matrix.append(news_matrix)\n",
    "        return np.array(res_matrix)\n",
    "    \n",
    "    # 处理价格数据\n",
    "    def process_price(self):\n",
    "        self.dow_jons_pd.drop(['High', 'Low', 'Close', 'Volume', 'Adj Close'],axis=1,inplace=True)\n",
    "        price_array = self.dow_jons_pd['Open']\n",
    "        self.dow_jons_pd['price_flag'] = [1 if price_array[i+1]>price_array[i] else 0 for i in range(self.dow_jons_pd.shape[0]-1)] + [0]\n",
    "        self.dow_jons_pd.drop(index=self.dow_jons_pd.shape[0]-1, axis=0,inplace=True)\n",
    "        self.dow_jons_pd['today_news'] = self.news_pd.Cleaned_News\n",
    "    # 主运行程序    \n",
    "    def run_engine(self):\n",
    "        self.clean_news_text()\n",
    "#         if os.path.isfile(self.save_model_file + \"WordVectorModel.bin\"):\n",
    "#             self.WordVectorModel = gensim.models.KeyedVectors.load_word2vec_format(self.save_model_file + 'WordVectorModel.bin',binary=True)\n",
    "#         else:    \n",
    "#             self.model_train()\n",
    "#         self.news_matrix = self.transfer_news_to_matrix()\n",
    "        self.process_price()\n",
    "        return self.dow_jons_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
